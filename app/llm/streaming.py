"""
LLM Streaming Support Module

This module provides streaming functionality for LLM chat models using
Server-Sent Events (SSE) and LangChain's streaming callback system.

Key Components:
- StreamingForwarderHandler: Callback handler that forwards tokens to an asyncio.Queue
- SSE formatting helpers for event streams
- Async generators for conversation and explanation streaming
"""

import asyncio
import json
import logging
import uuid
from dataclasses import dataclass
from typing import Any, AsyncGenerator, Dict, List, Optional, Union

from fastapi import Request
from langchain_community.chat_message_histories import SQLChatMessageHistory
from langchain_core.callbacks import BaseCallbackHandler
from langchain_core.messages import AIMessage, HumanMessage
from langchain_core.outputs import LLMResult

from app.llm.chains import ConversationChain
from app.llm.models import get_model_factory
from app.llm.prompts import get_chat_prompt_template, get_prompt_template

logger = logging.getLogger(__name__)


@dataclass
class StreamChunk:
    """Represents a chunk of streamed data"""

    type: str  # 'start', 'token', 'usage', 'end', 'error'
    content: Optional[str] = None
    usage: Optional[Dict[str, int]] = None
    message: Optional[str] = None
    model: Optional[str] = None
    session_id: Optional[str] = None
    meta: Optional[Dict[str, Any]] = None


class StreamingForwarderHandler(BaseCallbackHandler):
    """
    Callback handler that forwards LLM tokens to an asyncio.Queue for streaming.

    This handler captures tokens as they're generated by the LLM and puts them
    into a bounded queue that can be consumed by an async generator.
    """

    def __init__(self, queue: asyncio.Queue, max_queue_size: int = 100):
        super().__init__()
        self.queue = queue
        self.max_queue_size = max_queue_size
        self._cancelled = False

    def on_llm_start(
        self, serialized: Dict[str, Any], prompts: List[str], **kwargs: Any
    ) -> None:
        """Called when LLM starts running."""
        if not self._cancelled:
            try:
                # Extract model name from serialized data
                model_name = serialized.get("name", "unknown")
                chunk = StreamChunk(type="start", model=model_name)
                self.queue.put_nowait(chunk)
            except asyncio.QueueFull:
                logger.warning("Queue full, dropping start event")

    def on_llm_new_token(self, token: str, **kwargs: Any) -> None:
        """Called when a new token is generated."""
        if not self._cancelled and token:
            try:
                logger.debug(
                    f"StreamingForwarderHandler.on_llm_new_token token='{token[:40]}'"
                )
                chunk = StreamChunk(type="token", content=token)
                self.queue.put_nowait(chunk)
            except asyncio.QueueFull:
                logger.warning(f"Queue full, dropping token: {token[:20]}...")

    # LangChain 0.2+ ChatModel streaming callbacks (some providers use these instead of generic LLM callbacks)
    def on_chat_model_start(self, serialized: Dict[str, Any], messages: List[List[Any]], **kwargs: Any) -> None:  # type: ignore[override]
        if not self._cancelled:
            try:
                model_name = serialized.get("name", "unknown")
                self.queue.put_nowait(StreamChunk(type="start", model=model_name))
            except asyncio.QueueFull:
                logger.warning("Queue full, dropping chat_model start event")

    def on_chat_model_stream(self, chunk, **kwargs: Any) -> None:  # type: ignore[override]
        """Called for each streamed ChatModel chunk (AIMessageChunk)."""
        if self._cancelled:
            return
        try:
            # chunk may have .content (str or list); handle str only here
            content = getattr(chunk, "content", None)
            if isinstance(content, str) and content:
                logger.debug(
                    f"StreamingForwarderHandler.on_chat_model_stream content='{content[:40]}'"
                )
                self.queue.put_nowait(StreamChunk(type="token", content=content))
        except asyncio.QueueFull:
            logger.warning("Queue full, dropping chat_model token chunk")

    def on_chat_model_end(self, response, **kwargs: Any) -> None:  # type: ignore[override]
        if self._cancelled:
            return
        try:
            # Attempt to extract usage metadata if provided
            usage = None
            try:
                # OpenAI new client usage aggregation pattern (may differ)
                if hasattr(response, "usage_metadata") and response.usage_metadata:
                    md = response.usage_metadata
                    usage = {
                        "input_tokens": getattr(md, "input_tokens", 0),
                        "output_tokens": getattr(md, "output_tokens", 0),
                        "total_tokens": getattr(md, "total_tokens", 0),
                    }
            except Exception:  # pragma: no cover - defensive
                pass
            if usage:
                self.queue.put_nowait(StreamChunk(type="usage", usage=usage))
            self.queue.put_nowait(StreamChunk(type="end"))
        except asyncio.QueueFull:
            logger.warning("Queue full, dropping chat_model end events")

    def on_llm_end(self, response: LLMResult, **kwargs: Any) -> None:
        """Called when LLM finishes running."""
        if not self._cancelled:
            try:
                # Extract usage metadata if available
                usage = None
                if hasattr(response, "llm_output") and response.llm_output:
                    if "token_usage" in response.llm_output:
                        token_usage = response.llm_output["token_usage"]
                        usage = {
                            "input_tokens": token_usage.get("prompt_tokens", 0),
                            "output_tokens": token_usage.get("completion_tokens", 0),
                            "total_tokens": token_usage.get("total_tokens", 0),
                        }

                # Check for usage_metadata in generations (new LangChain format)
                if not usage and response.generations:
                    for generation_list in response.generations:
                        for generation in generation_list:
                            if hasattr(generation, "message") and hasattr(
                                generation.message, "usage_metadata"
                            ):
                                metadata = generation.message.usage_metadata
                                if metadata:
                                    usage = {
                                        "input_tokens": getattr(
                                            metadata, "input_tokens", 0
                                        ),
                                        "output_tokens": getattr(
                                            metadata, "output_tokens", 0
                                        ),
                                        "total_tokens": getattr(
                                            metadata, "total_tokens", 0
                                        ),
                                    }
                                    break
                        if usage:
                            break

                if usage:
                    usage_chunk = StreamChunk(type="usage", usage=usage)
                    self.queue.put_nowait(usage_chunk)

                end_chunk = StreamChunk(type="end")
                self.queue.put_nowait(end_chunk)
            except asyncio.QueueFull:
                logger.warning("Queue full, dropping end events")

    def on_llm_error(
        self, error: Union[Exception, KeyboardInterrupt], **kwargs: Any
    ) -> None:
        """Called when LLM encounters an error."""
        if not self._cancelled:
            try:
                error_chunk = StreamChunk(type="error", message=str(error))
                self.queue.put_nowait(error_chunk)
                end_chunk = StreamChunk(type="end")
                self.queue.put_nowait(end_chunk)
            except asyncio.QueueFull:
                logger.warning("Queue full, dropping error events")

    def cancel(self):
        """Cancel the handler to stop forwarding tokens."""
        self._cancelled = True


def format_sse(event_data: Dict[str, Any]) -> bytes:
    """
    Format data as Server-Sent Events (SSE) format.

    Args:
        event_data: Dictionary to be serialized as JSON

    Returns:
        Formatted SSE data as bytes
    """
    return f"data: {json.dumps(event_data, ensure_ascii=False)}\n\n".encode("utf-8")


async def stream_conversation(
    session_id: Optional[str],
    user_input: str,
    model_name: Optional[str] = None,
    request: Optional[Request] = None,
    **model_params,
) -> AsyncGenerator[bytes, None]:
    """
    Stream a conversation response using Server-Sent Events.

    Args:
        session_id: Conversation session ID (auto-generated if None)
        user_input: User's message
        model_name: Optional model override
        request: FastAPI request object for disconnect detection
        **model_params: Additional model parameters

    Yields:
        SSE-formatted bytes for each streaming event
    """
    # Generate session ID if not provided
    if not session_id:
        session_id = str(uuid.uuid4())

    logger.info(f"Starting conversation stream for session {session_id}")

    # Create bounded queue for token forwarding
    token_queue: asyncio.Queue = asyncio.Queue(maxsize=100)
    handler = StreamingForwarderHandler(token_queue)

    try:
        # Get model factory and create streaming model
        factory = get_model_factory()
        default_model_name = factory.config.get_task_model("conversation")

        # Create model with streaming enabled and callback handler
        model = factory.create_model(
            model_name or default_model_name,
            streaming=True,
            callbacks=[handler],
            **model_params,
        )

        if not model:
            error_data = {"event": "error", "message": "Failed to create model"}
            yield format_sse(error_data)
            return

        # Emit start event EARLY (before loading history) for low-latency first byte
        start_data = {
            "event": "start",
            "model": getattr(
                getattr(model, "model_name", None), "__str__", lambda: None
            )()
            or getattr(model, "model_name", None)
            or model_name
            or default_model_name,
            "session_id": session_id,
        }
        yield format_sse(start_data)

        # Load conversation history (can be I/O heavy) AFTER start was sent
        from app.core.config import get_settings

        settings = get_settings()
        url = settings.database_url
        if "+aiosqlite" in url:
            connection_string = url.replace("+aiosqlite", "")
        elif "+asyncpg" in url:
            connection_string = url.replace("+asyncpg", "+psycopg")
        elif "+aiomysql" in url:
            connection_string = url.replace("+aiomysql", "+pymysql")
        else:
            connection_string = url

        history = SQLChatMessageHistory(
            session_id=session_id, connection_string=connection_string
        )

        # Build message list manually for streaming
        messages = []

        # Add system message from prompt template
        prompt_template = get_chat_prompt_template("conversation")
        if prompt_template and hasattr(prompt_template, "messages"):
            for msg_template in prompt_template.messages:
                if hasattr(msg_template, "format"):
                    formatted = msg_template.format(user_input=user_input, history=[])
                    if formatted.type == "system":
                        from langchain_core.messages import SystemMessage

                        messages.append(SystemMessage(content=formatted.content))

        # Add conversation history
        history_messages = history.messages
        messages.extend(history_messages)

        # Add current user message
        messages.append(HumanMessage(content=user_input))

        # Start streaming using LangChain astream_events (preferred) with fallback to astream
        async def event_stream_model():
            """Consume LangChain structured events and forward token pieces to queue."""
            try:
                # Try events API first
                used_events_api = False
                if hasattr(model, "astream_events"):
                    try:
                        async for ev in model.astream_events(messages, version="v1"):
                            used_events_api = True
                            name = ev.get("event") if isinstance(ev, dict) else None
                            if not name:
                                continue
                            if name in ("on_chat_model_stream", "on_llm_new_token"):
                                data = (
                                    ev.get("data", {}) if isinstance(ev, dict) else {}
                                )
                                chunk_obj = (
                                    data.get("chunk")
                                    if isinstance(data, dict)
                                    else None
                                )
                                # Extract content from chunk_obj
                                content = getattr(chunk_obj, "content", None)
                                pieces: list[str] = []
                                if isinstance(content, str):
                                    pieces.append(content)
                                elif isinstance(content, list):
                                    for part in content:
                                        if isinstance(part, str):
                                            pieces.append(part)
                                        elif isinstance(part, dict):
                                            t = part.get("text") or part.get("content")
                                            if isinstance(t, str):
                                                pieces.append(t)
                                for piece in pieces:
                                    if not piece:
                                        continue
                                    try:
                                        token_queue.put_nowait(
                                            StreamChunk(type="token", content=piece)
                                        )
                                    except asyncio.QueueFull:
                                        logger.warning(
                                            "Queue full, dropping event token piece"
                                        )
                            elif name in ("on_chat_model_end", "on_llm_end"):
                                data = (
                                    ev.get("data", {}) if isinstance(ev, dict) else {}
                                )
                                # Attempt usage extraction
                                usage = None
                                output = (
                                    data.get("output")
                                    if isinstance(data, dict)
                                    else None
                                )
                                if (
                                    output
                                    and hasattr(output, "usage_metadata")
                                    and output.usage_metadata
                                ):
                                    md = output.usage_metadata
                                    usage = {
                                        "input_tokens": getattr(md, "input_tokens", 0),
                                        "output_tokens": getattr(
                                            md, "output_tokens", 0
                                        ),
                                        "total_tokens": getattr(md, "total_tokens", 0),
                                    }
                                if usage:
                                    try:
                                        token_queue.put_nowait(
                                            StreamChunk(type="usage", usage=usage)
                                        )
                                    except asyncio.QueueFull:
                                        pass
                                try:
                                    token_queue.put_nowait(StreamChunk(type="end"))
                                except asyncio.QueueFull:
                                    pass
                    except Exception as ev_err:  # fall back to astream below
                        logger.warning(
                            f"astream_events failed, falling back to astream: {ev_err}"
                        )
                if not used_events_api:
                    # Fallback: simple astream iteration (may buffer larger chunks)
                    async for chunk in model.astream(messages):
                        raw_content = getattr(chunk, "content", None)
                        parts: list[str] = []
                        if isinstance(raw_content, str):
                            parts.append(raw_content)
                        elif isinstance(raw_content, list):
                            for part in raw_content:
                                if isinstance(part, str):
                                    parts.append(part)
                                elif isinstance(part, dict):
                                    t = part.get("text") or part.get("content")
                                    if isinstance(t, str):
                                        parts.append(t)
                        for piece in parts:
                            if not piece:
                                continue
                            try:
                                token_queue.put_nowait(
                                    StreamChunk(type="token", content=piece)
                                )
                            except asyncio.QueueFull:
                                logger.warning(
                                    "Queue full, dropping fallback token piece"
                                )
                    # Ensure end if fallback path used
                    try:
                        token_queue.put_nowait(StreamChunk(type="end"))
                    except asyncio.QueueFull:
                        pass
            except Exception as e:
                logger.error(f"Error in model event streaming: {e}")
                try:
                    token_queue.put_nowait(StreamChunk(type="error", message=str(e)))
                    token_queue.put_nowait(StreamChunk(type="end"))
                except asyncio.QueueFull:
                    pass

        stream_task = asyncio.create_task(event_stream_model())

        # Process tokens from queue
        accumulated_content = ""
        while True:
            try:
                # Check if client disconnected
                if request and await request.is_disconnected():
                    logger.info("Client disconnected, stopping stream")
                    handler.cancel()
                    stream_task.cancel()
                    break

                # Wait for next chunk with timeout
                try:
                    chunk = await asyncio.wait_for(token_queue.get(), timeout=0.1)
                except asyncio.TimeoutError:
                    continue

                if chunk.type == "token" and chunk.content:
                    accumulated_content += chunk.content
                    token_data = {"event": "token", "content": chunk.content}
                    yield format_sse(token_data)

                elif chunk.type == "usage" and chunk.usage:
                    usage_data = {"event": "usage", "usage": chunk.usage}
                    yield format_sse(usage_data)

                elif chunk.type == "error":
                    error_data = {"event": "error", "message": chunk.message}
                    yield format_sse(error_data)
                    break

                elif chunk.type == "end":
                    # Stream completed successfully, persist history
                    if accumulated_content:
                        try:
                            # Add assistant message to history
                            ai_message = AIMessage(content=accumulated_content)
                            await asyncio.to_thread(history.add_message, ai_message)
                            logger.info(
                                f"Persisted assistant message to session {session_id}"
                            )
                        except Exception as e:
                            logger.error(f"Failed to persist history: {str(e)}")

                    end_data = {"event": "end"}
                    yield format_sse(end_data)
                    break

            except Exception as e:
                logger.error(f"Error processing stream chunk: {str(e)}")
                error_data = {"event": "error", "message": "Stream processing error"}
                yield format_sse(error_data)
                break

        # Ensure streaming task is cancelled
        if not stream_task.done():
            stream_task.cancel()
            try:
                await stream_task
            except asyncio.CancelledError:
                pass

    except Exception as e:
        logger.error(f"Error in conversation streaming: {str(e)}")
        error_data = {"event": "error", "message": str(e)}
        yield format_sse(error_data)

    finally:
        logger.info(f"Conversation stream ended for session {session_id}")


async def stream_explanation(
    question: str,
    model_name: Optional[str] = None,
    request: Optional[Request] = None,
    **model_params,
) -> AsyncGenerator[bytes, None]:
    """
    Stream an explanation response using Server-Sent Events.

    Args:
        question: Question to explain
        model_name: Optional model override
        request: FastAPI request object for disconnect detection
        **model_params: Additional model parameters

    Yields:
        SSE-formatted bytes for each streaming event
    """
    logger.info(f"Starting explanation stream for question: {question[:100]}...")

    # Create bounded queue for token forwarding
    token_queue: asyncio.Queue = asyncio.Queue(maxsize=100)
    handler = StreamingForwarderHandler(token_queue)

    try:
        # Get model factory and create streaming model
        factory = get_model_factory()
        default_model_name = factory.config.get_task_model("explain")

        # Create model with streaming enabled and callback handler
        model = factory.create_model(
            model_name or default_model_name,
            streaming=True,
            callbacks=[handler],
            **model_params,
        )

        if not model:
            error_data = {"event": "error", "message": "Failed to create model"}
            yield format_sse(error_data)
            return
        # Build prompt for explanation (system + user)
        prompt_template = get_prompt_template("explain")
        input_data = {"question": question}

        # Emit start event
        start_data = {"event": "start", "model": model_name or "explain"}
        yield format_sse(start_data)

        # Background task using events API with fallback
        async def event_stream_model():
            try:
                messages = []
                if prompt_template:
                    try:
                        formatted = prompt_template.format(**input_data)
                        if isinstance(formatted, str):
                            messages.append(HumanMessage(content=formatted))
                        else:
                            messages.append(HumanMessage(content=question))
                    except Exception:
                        messages.append(HumanMessage(content=question))
                else:
                    messages.append(HumanMessage(content=question))

                used_events_api = False
                if hasattr(model, "astream_events"):
                    try:
                        async for ev in model.astream_events(messages, version="v1"):
                            used_events_api = True
                            name = ev.get("event") if isinstance(ev, dict) else None
                            if not name:
                                continue
                            if name in ("on_chat_model_stream", "on_llm_new_token"):
                                data = (
                                    ev.get("data", {}) if isinstance(ev, dict) else {}
                                )
                                chunk_obj = (
                                    data.get("chunk")
                                    if isinstance(data, dict)
                                    else None
                                )
                                content = getattr(chunk_obj, "content", None)
                                parts: list[str] = []
                                if isinstance(content, str):
                                    parts.append(content)
                                elif isinstance(content, list):
                                    for part in content:
                                        if isinstance(part, str):
                                            parts.append(part)
                                        elif isinstance(part, dict):
                                            t = part.get("text") or part.get("content")
                                            if isinstance(t, str):
                                                parts.append(t)
                                for piece in parts:
                                    if not piece:
                                        continue
                                    try:
                                        token_queue.put_nowait(
                                            StreamChunk(type="token", content=piece)
                                        )
                                    except asyncio.QueueFull:
                                        logger.warning(
                                            "Queue full, dropping explain event token piece"
                                        )
                            elif name in ("on_chat_model_end", "on_llm_end"):
                                data = (
                                    ev.get("data", {}) if isinstance(ev, dict) else {}
                                )
                                usage = None
                                output = (
                                    data.get("output")
                                    if isinstance(data, dict)
                                    else None
                                )
                                if (
                                    output
                                    and hasattr(output, "usage_metadata")
                                    and output.usage_metadata
                                ):
                                    md = output.usage_metadata
                                    usage = {
                                        "input_tokens": getattr(md, "input_tokens", 0),
                                        "output_tokens": getattr(
                                            md, "output_tokens", 0
                                        ),
                                        "total_tokens": getattr(md, "total_tokens", 0),
                                    }
                                if usage:
                                    try:
                                        token_queue.put_nowait(
                                            StreamChunk(type="usage", usage=usage)
                                        )
                                    except asyncio.QueueFull:
                                        pass
                                try:
                                    token_queue.put_nowait(StreamChunk(type="end"))
                                except asyncio.QueueFull:
                                    pass
                    except Exception as ev_err:
                        logger.warning(
                            f"astream_events(explain) failed, fallback to astream: {ev_err}"
                        )
                if not used_events_api:
                    async for chunk in model.astream(messages):
                        raw_content = getattr(chunk, "content", None)
                        parts: list[str] = []
                        if isinstance(raw_content, str):
                            parts.append(raw_content)
                        elif isinstance(raw_content, list):
                            for part in raw_content:
                                if isinstance(part, str):
                                    parts.append(part)
                                elif isinstance(part, dict):
                                    t = part.get("text") or part.get("content")
                                    if isinstance(t, str):
                                        parts.append(t)
                        for piece in parts:
                            if not piece:
                                continue
                            try:
                                token_queue.put_nowait(
                                    StreamChunk(type="token", content=piece)
                                )
                            except asyncio.QueueFull:
                                logger.warning(
                                    "Queue full, dropping explain fallback token piece"
                                )
                    try:
                        token_queue.put_nowait(StreamChunk(type="end"))
                    except asyncio.QueueFull:
                        pass
            except Exception as e:
                logger.error(f"Error in explain model event streaming: {e}")
                try:
                    token_queue.put_nowait(StreamChunk(type="error", message=str(e)))
                    token_queue.put_nowait(StreamChunk(type="end"))
                except asyncio.QueueFull:
                    pass

        stream_task = asyncio.create_task(event_stream_model())

        # Consume queue and yield SSE
        while True:
            try:
                if request and await request.is_disconnected():
                    logger.info("Client disconnected, stopping stream")
                    handler.cancel()
                    stream_task.cancel()
                    break
                try:
                    chunk = await asyncio.wait_for(token_queue.get(), timeout=0.1)
                except asyncio.TimeoutError:
                    continue

                if chunk.type == "token" and chunk.content:
                    yield format_sse({"event": "token", "content": chunk.content})
                elif chunk.type == "usage" and chunk.usage:
                    yield format_sse({"event": "usage", "usage": chunk.usage})
                elif chunk.type == "error":
                    yield format_sse({"event": "error", "message": chunk.message})
                    break
                elif chunk.type == "end":
                    yield format_sse({"event": "end"})
                    break
            except Exception as e:
                logger.error(f"Error processing stream chunk: {str(e)}")
                yield format_sse(
                    {"event": "error", "message": "Stream processing error"}
                )
                break

        if not stream_task.done():
            stream_task.cancel()
            try:
                await stream_task
            except asyncio.CancelledError:
                pass

    except Exception as e:
        logger.error(f"Error in explanation streaming: {str(e)}")
        error_data = {"event": "error", "message": str(e)}
        yield format_sse(error_data)

    finally:
        logger.info("Explanation stream ended")
